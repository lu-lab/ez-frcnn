{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "349afa44",
   "metadata": {},
   "source": [
    "---\n",
    "## DEMO: Applying a cascade of two EZ-FRCNN models to analyze an example video of a single worm\n",
    "In this demo, you will use two pre-trained EZ-FRCNN models* to analyze a video of feeding *C. elegans* on food. The steps are as follows:\n",
    "1. Apply the pharyngeal bulb-tracking EZ-FRCNN model to the original 4k video.\n",
    "2. Use the tracked bulb location to dynamically crop the original 4k video into a 250x250p video centered on the bulb.\n",
    "3. Apply the grinder-tracking EZ-FRCNN model to the 250x250p video centered on the grinder.\n",
    "\n",
    ">*NOTE: You will need to download the models from our OSF, located [here](https://osf.io/z7t9s/). Navigate to the Files tab, then click `c. elegans grinder tracking/models` to locate and download each model. After downloading, move the models to the `demo_wormmtg/models` folder within this repository.\n",
    "\n",
    ">Example videos are also available on [OSF](https://osf.io/z7t9s/) to download under `c. elegans grinder tracking/videos/demo_wormmtg`\n",
    "\n",
    "### Motivation\n",
    "We use EZ-FRCNN both as a preprocessing step as well as an object detector in tracking the grinder in freely moving *C. elegans*. In a 4K video taken at 6X magnification on a 10X objective, the grinder appears as a dark, 20x20 pixel \"blob\" that can be difficult to track in the raw video data. To circumvent this, we first train an EZ-FRCNN model to track the pharyngeal bulb: the part of the pharynx that contains the grinder. Then, we use this bulb location to crop an ROI centered on the grinder. Using this ROI video, we can then apply a 2nd EZ-FRCNN model to track the grinder reliably."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3e5a09f9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# --- LOAD LIBRARIES + CHECK THAT GPU IS AVAILABLE ---\n",
    "from library import *     # imports all libraries needed to run this code\n",
    "torch.cuda.is_available() # check that the GPU is available, will print 'True' if it is \n",
    "                          # (not REQUIRED, but highly RECOMMENDED)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1c709fa",
   "metadata": {},
   "source": [
    "---\n",
    "### Step 1: Apply pharyngeal bulb tracking\n",
    "This step will apply a pretrained bulb tracking EZ-FRCNN model to a video. The output of this step is as follows: \n",
    "1. A CSV listing out the bounding box location of the bulb at each frame of the video, located at `demo_wormmtg/VIDEONAME_bulb_boxes.csv`\n",
    "2. A labeled version of the video, located at `demo_wormmtg/VIDEONAME_labeled.avi`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "53b1d213",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- LOAD PRE-TRAINED BULB TRACKING MODEL ---\n",
    "detection_threshold = 0.7 # this threshold determines how high the confidence score for an \n",
    "                          # inference must be to be considered a correct label\n",
    "\n",
    "MODEL_DIR   = './demo_wormmtg/models'\n",
    "CLASSES     = ['background','bulb']\n",
    "NUM_CLASSES = 2\n",
    "model_name  = 'example_bulb_model.pth'\n",
    "model       = load_model(model_name, MODEL_DIR, NUM_CLASSES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c597a77d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Video saved to  ./demo_wormmtg/outputs/example_video_onfood_labeled.avi\n"
     ]
    }
   ],
   "source": [
    "# --- APPLY PRE-TRAINED BULB TRACKING MODEL TO VIDEO ---\n",
    "video        = 'example_video_onfood'\n",
    "video_name   = video + '.wmv'\n",
    "VID_DIR      = './demo_wormmtg/videos/' + video_name\n",
    "OUT_VID_DIR  = './demo_wormmtg/outputs/' + video + '_labeled.avi'\n",
    "BULB_BOX_DIR = './demo_wormmtg/outputs/' + video + '_bulb_boxes.csv'\n",
    "\n",
    "# This function will apply the model to the video\n",
    "[bboxes, classes, scores] = inference_video_clean(VID_DIR, model, detection_threshold, CLASSES)\n",
    "\n",
    "# This function will save the bounding box locations\n",
    "saveBestBoxes(BULB_BOX_DIR, bboxes)\n",
    "\n",
    "# This function will generate a video with the bounding boxes labeled\n",
    "save_clean_video(VID_DIR, BULB_BOX_DIR, OUT_VID_DIR, lw=3, color=(51, 114, 252))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7db13150",
   "metadata": {},
   "source": [
    "---\n",
    "### Step 2: Crop the original video using the tracked pharyngeal bulb\n",
    "This step will use the tracked bulb from Step 1 to crop an ROI from each frame of the selected video. The output of this step is as follows: \n",
    "1. A cropped version of the original video, located at `demo_wormmtg/videos/VIDEONAME_cropped.avi`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "566227db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved to ./demo_wormmtg/videos/example_video_onfood_cropped.wmv\n"
     ]
    }
   ],
   "source": [
    "# --- PARAMETERS ---\n",
    "ROI_width   = 125 # resulting video will be 2*roi_width x 2*roi_width\n",
    "thresh, sig = 250, 6 # threshold for filtering ROI coordinates, sigma used for smoothing the ROI centers\n",
    "frame_size  = 2 * ROI_width, 2 * ROI_width\n",
    "\n",
    "# --- LOAD BOUNDING BOXES & VIDEO ---\n",
    "boxes = pd.read_csv(BULB_BOX_DIR).to_numpy()\n",
    "vid   = cv2.VideoCapture(VID_DIR)\n",
    "width, height = int(vid.get(3)), int(vid.get(4))\n",
    "fps   = vid.get(cv2.CAP_PROP_FPS)\n",
    "tot_frames = int(vid.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "\n",
    "# --- OUTPUT SETUP ---\n",
    "out_path = './demo_wormmtg/videos/' + video + '_cropped.wmv'\n",
    "vwriter = cv2.VideoWriter(out_path, cv2.VideoWriter_fourcc(*'MJPG'), fps, frame_size)\n",
    "\n",
    "# --- COMPUTE ROI CENTERS ---\n",
    "ROI_coords = np.array([\n",
    "    [(box[0] + box[2]) // 2, (box[1] + box[3]) // 2]\n",
    "    for box in boxes[:tot_frames - 1]\n",
    "])\n",
    "\n",
    "# --- FILTER & SMOOTH COORDS ---\n",
    "ROI_coords_i, ROI_coords_s = filter_ROIs(ROI_coords, threshold=thresh, sigma=sig, max_values=(3840, 2160))\n",
    "\n",
    "# --- PROCESS VIDEO ---\n",
    "for f in range(1, tot_frames-1):\n",
    "    success, frame = vid.read()\n",
    "    if not success: break\n",
    "\n",
    "    print(f\"frame: {f-1}\")\n",
    "    ROI = getROI(ROI_width, ROI_coords_s[f], frame, width, height)\n",
    "    vwriter.write(ROI)\n",
    "    clear_output(wait=True)\n",
    "\n",
    "# --- CLEANUP ---\n",
    "vwriter.release()\n",
    "clear_output(wait=True)\n",
    "print(\"Saved to\", out_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20b6c2a7",
   "metadata": {},
   "source": [
    "---\n",
    "### Step 3: Apply grinder tracking\n",
    "This step will apply a pretrained grinder tracking EZ-FRCNN model to an ROI video generated in Step 2. The output of this step is as follows: \n",
    "1. A CSV listing out the bounding box location of the grinder at each frame of the video, located at `demo_wormmtg/VIDEONAME_grinder_boxes.csv`\n",
    "2. A labeled version of the ROI video, located at `demo_wormmtg/VIDEONAME_cropped_labeled.avi`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b614bda8",
   "metadata": {},
   "outputs": [],
   "source": [
    "### LOAD PRE-TRAINED GRINDER TRACKING MODEL\n",
    "detection_threshold = 0.5 # this threshold determines how high the confidence score for an \n",
    "                          # inference must be to be considered a correct label\n",
    "CLASSES    = ['background','grinder']\n",
    "model_name = 'example_grinder_model.pth'\n",
    "model      = load_model(model_name, MODEL_DIR, NUM_CLASSES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ffa17581",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "### APPLY PRE-TRAINED GRINDER TRACKING MODEL TO CROPPED VIDEO\n",
    "video           = 'example_video_onfood'\n",
    "video_name      = video + '_cropped.wmv'\n",
    "VID_DIR         = './demo_wormmtg/videos/' + video_name\n",
    "OUT_VID_DIR     = './demo_wormmtg/outputs/' + video + '_cropped_labeled.avi'\n",
    "GRINDER_BOX_DIR = './demo_wormmtg/outputs/' + video + '_grinder_boxes.csv'\n",
    "\n",
    "# This function will apply the model to the video\n",
    "[bboxes, classes, scores] = inference_video_clean(VID_DIR, model, detection_threshold, CLASSES)\n",
    "\n",
    "# This function will save the bounding box locations\n",
    "saveBestBoxes(GRINDER_BOX_DIR, bboxes)\n",
    "\n",
    "# This function will generate a video with the bounding boxes labeled\n",
    "save_clean_video(VID_DIR, GRINDER_BOX_DIR, OUT_VID_DIR, lw=2, color=(51, 114, 252))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97359fc1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
