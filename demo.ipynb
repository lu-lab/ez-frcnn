{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "49cc749d-db12-4db0-98a4-bc7db9638cd7",
   "metadata": {},
   "source": [
    "# ðŸ¸ EZ-FRCNN Demo: First-Time Use with Preloaded Frog Dataset\n",
    "\n",
    "Welcome to the EZ-FRCNN demo notebook! This walkthrough will guide you through:\n",
    "\n",
    "- âœ… Training an object detection model on a **preloaded frog dataset**\n",
    "- âœ… Running inference on a held-out test set\n",
    "- âœ… Visualizing and exporting detection results\n",
    "\n",
    "This notebook is built for **first-time users** â€” no setup or data prep is required beyond running the cells.\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸ“ Preloaded Dataset\n",
    "\n",
    "The demo dataset is already included and organized as follows:\n",
    "\n",
    "- `images/train` â€” training images and annotations  \n",
    "- `images/test` â€” test images and annotations\n",
    "\n",
    "> âœ… No need to modify or download anything. The data is ready to use.\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸš€ How to Use This Notebook\n",
    "\n",
    "1. Run all cells in order (top to bottom)\n",
    "2. The model will train on `images/train`and validate on `images/test`\n",
    "3. Inference will run on `test_data/test_images`\n",
    "4. Detections will be visualized and saved\n",
    "\n",
    "This demo confirms your EZ-FRCNN installation works and gives you a complete example pipeline.\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸ’¡ Try EZ-FRCNN on Your Own Images\n",
    "\n",
    "Once the demo completes, you can run inference on your own images using the `ezfrcnn.ipynb` notebook\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "241f4862",
   "metadata": {},
   "outputs": [],
   "source": [
    "from library import *\n",
    "from tqdm.auto import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4e98e26-6acb-4c6a-a427-ea60f2ecba76",
   "metadata": {},
   "source": [
    "Training parameters are defined below, no need to change these on the first run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97330dbf-84b1-4436-8121-6cb49d110b1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 4 # increase / decrease according to GPU memory\n",
    "RESIZE_TO = 512 # resize the image for training and transforms\n",
    "NUM_EPOCHS = 20 # number of epochs to train for\n",
    "SAVE_PLOTS_EPOCH = 1 # save loss plots after these many epochs\n",
    "SAVE_MODEL_EPOCH = 5 # save model after these many epochs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ca115da-ad04-443d-80b2-8a68715a0d0c",
   "metadata": {},
   "source": [
    "Run the cell below to load in your images and annotations. This cell will print the number of images found in the testing and training folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b9a31c2-b08f-4d9d-9f00-165752746de5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare the final datasets and data loaders\n",
    "train_dataset = getDataset(TRAIN_DIR, RESIZE_TO, RESIZE_TO, get_train_transform())\n",
    "valid_dataset = getDataset(VALID_DIR, RESIZE_TO, RESIZE_TO, get_valid_transform())\n",
    "[train_loader, valid_loader] = get_loaders(train_dataset, valid_dataset, BATCH_SIZE, collate_fn)\n",
    "\n",
    "NUM_CLASSES = len(train_dataset.classes)\n",
    "\n",
    "print(f\"Number of training samples: {len(train_dataset)}\")\n",
    "print(f\"Number of validation samples: {len(valid_dataset)}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f3967df-913e-4600-ab22-9f4c487ff8eb",
   "metadata": {},
   "source": [
    "Run the cell below to visualize your data to ensure that bounding boxes match images as expected. There will be gridlines in the image shown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee0d0b86-a46e-45a5-9442-d18f092ef186",
   "metadata": {},
   "outputs": [],
   "source": [
    "INDEX_TO_VIS = 0\n",
    "print(train_dataset.classes)\n",
    "visualize_sample(TRAIN_DIR, RESIZE_TO, INDEX_TO_VIS)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6751efa5",
   "metadata": {},
   "source": [
    "To start training from the default COCO weights run the cell below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b14da72",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = create_model(num_classes=NUM_CLASSES)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "240ecb80-d2d4-45c2-9166-a7120ed8f7dc",
   "metadata": {},
   "source": [
    "Train your model by running the cell below! After the specified number of epochs passes, the model and a plot of the training/validation loss over time will be saved under `/models` and `/plots` respectively. This should take around 10 minutes on a device with a GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24d9c754-78dc-4524-8be3-8904db9c8455",
   "metadata": {},
   "outputs": [],
   "source": [
    "DEVICE = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "# name to save the trained model with\n",
    "MODEL_NAME = 'model'\n",
    "tq1 = tqdm(range(0,NUM_EPOCHS))\n",
    "fig = plt.figure\n",
    "[train_loss_list, val_loss_list] = train_model(model, train_loader, valid_loader,\n",
    "                                               DEVICE, MODEL_NAME, NUM_EPOCHS,\n",
    "                                               MODEL_DIR, PLOT_DIR, SAVE_MODEL_EPOCH,\n",
    "                                               SAVE_PLOTS_EPOCH, tq1, fig)\n",
    "# Plot Validation & Training Loss\n",
    "plt.plot(train_loss_list)\n",
    "plt.plot(val_loss_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20552cb8-dc8d-4682-86ea-3c3c3174b1ea",
   "metadata": {},
   "source": [
    "<h1> Inferencing</h1>\n",
    "Run the cells below to create bounding boxes for new data. Images with bounding boxes overlaid will be present in the outputs folder following inferencing as well. CSV files will also be created, one for the class of each object detected and one for the bounding boxes of said objects. Each row of the CSV file will correspond to each frame of the movie inferenced (in the case of movie inferencing) or each image in the ordering defined by the output images (in the case of image inferencing). Each bounding box is of the form [xmin ymin xmax ymax]\n",
    "<br> <br>\n",
    "First define the confidence threshold you would like to use for inferencing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc8c3d44-0155-4bac-92d4-544e0bfca4d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "detection_threshold = 0.9# 0.9 by default"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "270c1551-eb3a-4de7-92a4-4eedc34e8944",
   "metadata": {},
   "source": [
    "Load your model by running the cell below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fb6cc71-023a-4201-97ec-4adc6caa0914",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'model20.pth'\n",
    "model = load_model(model_name, MODEL_DIR, NUM_CLASSES)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c52f6aa",
   "metadata": {},
   "source": [
    "Finally, run inferencing here: images with bounding boxes overlaid and a csv of the detections will be output in `/outputs`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3576357d-09b9-4ac9-b557-4ef12be0bc37",
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Path to folder of images for inferencing\n",
    "folderName = './test_data/test_images/'\n",
    "[boxFileName, classFileName, scoreFileName] = ['boxes', 'classes', 'scores']\n",
    "inf_fig = plt.figure()\n",
    "results = inference_images(folderName, model, OUT_DIR, detection_threshold, train_dataset.classes, tqdm, inf_fig)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
