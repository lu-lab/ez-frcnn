{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "49cc749d-db12-4db0-98a4-bc7db9638cd7",
   "metadata": {},
   "source": [
    "<h1> Training</h1>\n",
    "If you have annotations from the previous version of FRCNN, go to the 'convertFromCSV' notebook and follow the provided instructions. After doing so, return to this notebook.\n",
    "<br> <br>\n",
    "Before training, split your data into some training images and some validation images. To do this, add each training image as well as its corresponding annotation .xml file into the images/train folder in the FRCNN2 folder. Similarly, add each validation image as well as its corresponding annotation .xml file into the images/test folder. A 90% training image, 10% validation image split is recommended\n",
    "<br> <br>\n",
    "Run the cell below to define all imports and helper functions for this notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "241f4862",
   "metadata": {},
   "outputs": [],
   "source": [
    "from library import *\n",
    "from tqdm.auto import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dcef66a-d149-4725-8e91-33962f0d6054",
   "metadata": {},
   "source": [
    "Populate CLASSES with the classes you would like to find. As indictated, reserve the first entry for the background. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4e98e26-6acb-4c6a-a427-ea60f2ecba76",
   "metadata": {},
   "source": [
    "Training parameters are defined below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97330dbf-84b1-4436-8121-6cb49d110b1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 4 # increase / decrease according to GPU memory\n",
    "RESIZE_TO = 512 # resize the image for training and transforms\n",
    "NUM_EPOCHS = 5 # number of epochs to train for\n",
    "SAVE_PLOTS_EPOCH = 1 # save loss plots after these many epochs\n",
    "SAVE_MODEL_EPOCH = 5 # save model after these many epochs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ca115da-ad04-443d-80b2-8a68715a0d0c",
   "metadata": {},
   "source": [
    "Run the cell below to load in your images and annotations. This cell will print the number of images found in the testing and training folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b9a31c2-b08f-4d9d-9f00-165752746de5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare the final datasets and data loaders\n",
    "train_dataset = getDataset(TRAIN_DIR, RESIZE_TO, RESIZE_TO, get_train_transform())\n",
    "valid_dataset = getDataset(VALID_DIR, RESIZE_TO, RESIZE_TO, get_valid_transform())\n",
    "[train_loader, valid_loader] = get_loaders(train_dataset, valid_dataset, BATCH_SIZE, collate_fn)\n",
    "\n",
    "NUM_CLASSES = len(train_dataset.classes)\n",
    "\n",
    "print(f\"Number of training samples: {len(train_dataset)}\")\n",
    "print(f\"Number of validation samples: {len(valid_dataset)}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f3967df-913e-4600-ab22-9f4c487ff8eb",
   "metadata": {},
   "source": [
    "Run the cell below to visualize your data to ensure that bounding boxes match images as expected. There will be gridlines in the image shown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee0d0b86-a46e-45a5-9442-d18f092ef186",
   "metadata": {},
   "outputs": [],
   "source": [
    "INDEX_TO_VIS = 0\n",
    "print(train_dataset.classes)\n",
    "visualize_sample(TRAIN_DIR, RESIZE_TO, INDEX_TO_VIS)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6751efa5",
   "metadata": {},
   "source": [
    "To start training from the default COCO weights run the cell below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b14da72",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = create_model(num_classes=NUM_CLASSES)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8bb793f",
   "metadata": {},
   "source": [
    "To load a model to continue training run the cell below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fa51f58",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'model5.pth'\n",
    "model = load_model_train(model_name, MODEL_DIR, NUM_CLASSES=NUM_CLASSES)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "240ecb80-d2d4-45c2-9166-a7120ed8f7dc",
   "metadata": {},
   "source": [
    "Train your model by running the cell below! After the specified number of epochs passes, the model will be saved and a plot of the training/validation loss over time will be shown "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24d9c754-78dc-4524-8be3-8904db9c8455",
   "metadata": {},
   "outputs": [],
   "source": [
    "DEVICE = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "# name to save the trained model with\n",
    "MODEL_NAME = 'model'\n",
    "tq1 = tqdm(range(0,NUM_EPOCHS))\n",
    "fig = plt.figure\n",
    "[train_loss_list, val_loss_list] = train_model(model, train_loader, valid_loader,\n",
    "                                               DEVICE, MODEL_NAME, NUM_EPOCHS,\n",
    "                                               MODEL_DIR, PLOT_DIR, SAVE_MODEL_EPOCH,\n",
    "                                               SAVE_PLOTS_EPOCH, tq1, fig)\n",
    "# Plot Validation & Training Loss\n",
    "plt.plot(train_loss_list)\n",
    "plt.plot(val_loss_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20552cb8-dc8d-4682-86ea-3c3c3174b1ea",
   "metadata": {},
   "source": [
    "<h1> Inferencing</h1>\n",
    "Run the cells below to create bounding boxes for new data. Images with bounding boxes overlaid will be present in the outputs folder following inferencing as well. CSV files will also be created, one for the class of each object detected and one for the bounding boxes of said objects. Each row of the CSV file will correspond to each frame of the movie inferenced (in the case of movie inferencing) or each image in the ordering defined by the output images (in the case of image inferencing). Each bounding box is of the form [xmin ymin xmax ymax]\n",
    "<br> <br>\n",
    "First define the confidence threshold you would like to use for inferencing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc8c3d44-0155-4bac-92d4-544e0bfca4d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "detection_threshold = 0.2# 0.9 by default"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "270c1551-eb3a-4de7-92a4-4eedc34e8944",
   "metadata": {},
   "source": [
    "Load your model by running the cell below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fb6cc71-023a-4201-97ec-4adc6caa0914",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'model5.pth'\n",
    "model = load_model(model_name, MODEL_DIR, NUM_CLASSES)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c52f6aa",
   "metadata": {},
   "source": [
    "<h2> Image Inferencing </h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3576357d-09b9-4ac9-b557-4ef12be0bc37",
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Path to folder of images for inferencing\n",
    "folderName = './test_data/test_images/'\n",
    "[boxFileName, classFileName, scoreFileName] = ['boxes', 'classes', 'scores']\n",
    "inf_fig = plt.figure()\n",
    "results = inference_images(folderName, model, OUT_DIR, detection_threshold, train_dataset.classes, tqdm, inf_fig)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71508a5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path to folder of images for inferencing\n",
    "folderName = './test_data/test_images/'\n",
    "[boxFileName, classFileName, scoreFileName] = ['boxes', 'classes', 'scores']\n",
    "results = inference_images_fast(folderName, model, OUT_DIR, detection_threshold, train_dataset.classes, tqdm,\n",
    "                                batch_size=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85bdc43f-e231-4657-bc61-9e50efe6a1bc",
   "metadata": {},
   "source": [
    "<h2> Movie Inferencing</h2>\n",
    "Run inferencing on your data by running the cell below. This will output a video to 'outpy.avi' which overlays bounding boxes with their respective classes. The variables 'bboxes' and 'classes' contain inferencing information for each frame. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5cf5245-c27c-493a-8452-82f3ba03eadc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path to video for inferencing\n",
    "vidName = 'agar_4.avi'\n",
    "DIR_TEST = './test_data/' + vidName\n",
    "# Output file name\n",
    "outputName = 'outpy.avi'\n",
    "\n",
    "[boxFileName, classFileName, scoreFileName] = ['boxes', 'classes', 'scores']\n",
    "[bboxes, classes, scores] = inference_video(DIR_TEST, OUT_DIR, outputName, model, detection_threshold, CLASSES, save_detections=True)\n",
    "saveBoxesClassesScores(boxFileName, classFileName, scoreFileName, bboxes, classes, scores, OUT_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc19f860",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
